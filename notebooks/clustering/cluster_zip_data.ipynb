{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handgeschriebene Ziffern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der beiliegenden Beschreibung der Daten habe ich entnommen, dass es sich um handgeschriebene Ziffern handelt. Die mit 16x16 Bildpunkten abgespeichert sind. Der Beschreibung kann ich weiter entnehmen:\n",
    "\n",
    "*\"The data are in two gzipped files, and each line consists of the digit\n",
    "id (0-9) followed by the 256 grayscale values.\"*\n",
    "\n",
    "Es handelt sich also um eine Textdatei in der in jeder Zeile der Wert der geschriebenen Ziffer sowie die 16x16 = 256 Bildpunkte stehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=10, covariance_type='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumber(data, digit):\n",
    "    # wähle alle Zeilen in denen die erste Spalte (Spalte mit den Ziffer) mit 'digit' übereinstimmt\n",
    "    data_number = pd.DataFrame(data[data[0] == digit]) \n",
    "    # schneide die erste Spalte weg und gebe den rest zurück\n",
    "    separated_data = data_number.iloc[:,1:]\n",
    "    return separated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumberImage(data, digit, aggregation):\n",
    "    # ruft die Hilfsfunktion getNumber auf um alle Bilddaten Daten zu einer Ziffer zu holen\n",
    "    image_data = getNumber(data, digit)\n",
    "    # bilde aus allen Bildern zu der einen Ziffer ein aggrigiertes Bild\n",
    "    if(aggregation == 'median'):\n",
    "        df1 = pd.DataFrame(image_data.median())\n",
    "    else:\n",
    "        df1 = pd.DataFrame(image_data.mean())        \n",
    "    \n",
    "    # wandle das format von (1,256) -> (16,16) und gebe diesen transformierten DataFrame zurück\n",
    "    return df1.values.reshape(16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, targets):\n",
    "        self.results = pd.DataFrame(targets, columns=[\"targets\"])\n",
    "\n",
    "        self.input_data = {}\n",
    "        \n",
    "        self.names = []        \n",
    "        self.configs = []\n",
    "\n",
    "        \n",
    "    def get_names(self):\n",
    "        return self.names\n",
    "    \n",
    "    \n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "    \n",
    "    def get_config_map(self):\n",
    "        for conf in self.configs:\n",
    "            print (\"CONFIG:\", conf['name'], conf['input_data_name'], \"\\tNum features:\", self.input_data[conf['input_data_name']]['data'].shape[1])\n",
    "    \n",
    "    \n",
    "    def add_input_data(self, name, data, description=\"\"):\n",
    "        self.input_data[name] = {'description': description, 'data': data}\n",
    "    \n",
    "    \n",
    "    def add_config(self, algo, name, input_data_name):\n",
    "        # TODO: add check for dublicate names\n",
    "        if input_data_name not in self.input_data:\n",
    "            print (\"ERROR: feature_set_name not found\")\n",
    "            return \n",
    "        \n",
    "        self.names.append(name)\n",
    "        self.configs.append({ 'name': name, 'algo': algo, 'input_data_name': input_data_name, 'fit': -1})\n",
    "        \n",
    "\n",
    "    def _fit(self, conf):\n",
    "        # get features\n",
    "        input_data = self.input_data[conf['input_data_name']]['data']\n",
    "\n",
    "        # fit the features\n",
    "        print (\"Fitting \", conf['name'], \"... \",)\n",
    "        \n",
    "        start = time.time() # startzeit\n",
    "        conf['fit'] = conf['algo'].fit(input_data)\n",
    "        end = time.time() # startzeit\n",
    "        \n",
    "        print (\"done in \", end - start , \"s.\")\n",
    "\n",
    "        \n",
    "    def _predict(self, conf, data):\n",
    "        if data.shape[1] != self.input_data[conf['input_data_name']]['data'].shape[1]:\n",
    "            print (\"ERROR: Number of input features does not match (\", conf['name'], \")\")\n",
    "            return\n",
    "        \n",
    "        print (\"Predicting data with\", conf['name'], \"... \",)\n",
    "        \n",
    "        start = time.time() # startzeit\n",
    "        self.results[conf['name']] = conf['algo'].predict(data)\n",
    "        end = time.time() # startzeit\n",
    "        \n",
    "        print (\"done in \", end - start , \"s.\" )\n",
    "\n",
    "        \n",
    "    def fit(self):\n",
    "        '''Only trains algorithms if no fit has been calculated before'''\n",
    "        for conf in self.configs:\n",
    "            if conf['fit'] == -1:\n",
    "                self._fit(conf)\n",
    "\n",
    "                \n",
    "    def fit_all(self, data):\n",
    "        '''Trains all algorithms with given data'''\n",
    "        for conf in self.configs:\n",
    "            self._fit(conf)\n",
    "\n",
    "            \n",
    "    def refit(self, data):\n",
    "        '''Retrains all algorithms with given data that have been trained before'''\n",
    "        for conf in self.configs:\n",
    "            if conf['fit'] != -1:\n",
    "                self._fit(conf)\n",
    "\n",
    "                \n",
    "    def predict(self, name, data):\n",
    "        for conf in self.configs:\n",
    "            if conf['fit'] != -1 and conf['name'] == name:\n",
    "                self._predict(conf, data)\n",
    "\n",
    "    def predict_all(self, data):\n",
    "        for conf in self.configs:\n",
    "            if conf['fit'] != -1:\n",
    "                self._predict(conf, data)\n",
    "\n",
    "                \n",
    "    def get_count_df(self, name):\n",
    "        count_matrix = np.zeros((10,10)) \n",
    "        for digit in range(0,10):\n",
    "            cluster_counts = self.results[self.results['targets'] == digit].groupby(name).count()['targets']\n",
    "            for cluster in cluster_counts.keys():\n",
    "                count_matrix[digit][cluster] = cluster_counts[cluster]\n",
    "        \n",
    "        col_names = []\n",
    "        for i in range(0,10):\n",
    "            col_names.append(\"C_\" + str(i))\n",
    "    \n",
    "        return pd.DataFrame(count_matrix, columns=col_names)\n",
    "\n",
    "    def get_norm_df(self, name):\n",
    "        count_df = self.get_count_df(name)\n",
    "        return count_df.divide(count_df.sum(1), axis=0)\n",
    "    \n",
    "    \n",
    "    def get_all_error_rates(self):\n",
    "        error_rates = {}\n",
    "        for conf in self.configs:\n",
    "            if (conf['fit'] != -1) & (conf['name'] in self.results.columns):\n",
    "                error_rates[conf['name']] = self.get_error_rate(conf['name'])\n",
    "        return error_rates\n",
    "                \n",
    "    def get_error_rate(self, name):\n",
    "        '''Determine classification error by identifying best fitting column assignment'''\n",
    "        \n",
    "        normalized_counts = self.get_norm_df(name)\n",
    "        cluster_assignmend = normalized_counts.values.argmax(axis=1)\n",
    "    \n",
    "        norm_np = normalized_counts.values\n",
    "    \n",
    "        # start with custer assignment using max function, which may contain doublicates\n",
    "        new_order = normalized_counts.values.argmax(axis=1)\n",
    "        unique_cluster_ids, unique_idx, inverse_map, cluster_counts = np.unique(new_order, return_counts=True, return_inverse=True, return_index=True)\n",
    "        \n",
    "        # determine unassigned cluster ids\n",
    "        unprecise_clusters = []\n",
    "        idx = 0\n",
    "\n",
    "        for i in range(0,10):\n",
    "            if i >= len(unique_cluster_ids):\n",
    "                unprecise_clusters.append(i) \n",
    "                continue\n",
    "            if unique_cluster_ids[idx] != i:\n",
    "                unprecise_clusters.append(i)   \n",
    "            else:\n",
    "                idx += 1\n",
    "\n",
    "        # determine unclean clusters\n",
    "        confused_digits = []\n",
    "        for i in range(0, len(unique_cluster_ids)):\n",
    "            # skip if no dublicate exists\n",
    "            if cluster_counts[i] == 1:\n",
    "                continue\n",
    "            \n",
    "            # case where clusters were assigned to multiple digits\n",
    "            unprecise_clusters.append(new_order[unique_idx[i]])\n",
    "            for j in range(unique_idx[i], 10):\n",
    "                if new_order[j] == unique_cluster_ids[i]:\n",
    "                    confused_digits.append(j)\n",
    "        \n",
    "        \n",
    "        # set corresponding digits\n",
    "        idx = 0\n",
    "        for digit in confused_digits:\n",
    "            new_order[digit] = unprecise_clusters[idx]\n",
    "            idx += 1\n",
    "             \n",
    "        \n",
    "        # calculate maximal weight in regard to sum of frequencies \n",
    "        weight = normalized_counts.reindex(normalized_counts.columns[new_order], axis=1).values.diagonal().sum()\n",
    "        max_weight = weight\n",
    "        max_perm = new_order.copy()\n",
    "        for i in range(0, len(confused_digits)):\n",
    "            for j in range(i + 1, len(confused_digits)):  \n",
    "                #permutate entires\n",
    "                perm = new_order.copy()\n",
    "                perm[confused_digits[i]] = new_order[confused_digits[j]]\n",
    "                perm[confused_digits[j]] = new_order[confused_digits[i]]\n",
    "                \n",
    "                # calculate diagonal weights\n",
    "                weight = normalized_counts.reindex(normalized_counts.columns[perm], axis=1).values.diagonal().sum()\n",
    "                if weight > max_weight:\n",
    "                    max_weight = weight\n",
    "                    max_perm = perm.copy()\n",
    "        \n",
    "        perm_inv = np.zeros(len(max_perm),dtype=int)\n",
    "        for i in range(0, len(max_perm)):\n",
    "            perm_inv[max_perm[i]] = i\n",
    "        \n",
    "        \n",
    "        self.results[name + '_TARGET'] = self.results[[name]].apply(lambda x: perm_inv[x])\n",
    "        return 1.0 - (1.0 * (self.results['targets'] == self.results[name + '_TARGET']).sum()) / self.results.shape[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mit header=None, wird die erste Zeile der Datei nicht als Header interpretiert \n",
    "#              (Man könnte den Header in einem solchen Fall als Zeile mit den Spaltenüberschriften bezeichnen)\n",
    "# mit sep=\" \", geben wir an, dass wir das Leerzeichen als Seperator verwenden wollen. \n",
    "#              D.h. zwei durch ein Leerzeichen separierte Werte sollen als zwei Werte eingelesen werden.\n",
    "data = pd.read_csv(\"../data/zip.train\", header=None, sep=\" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = data.dropna(axis=1, thresh=2) # lass alle Spalten mit mehr als 2 NaN (Not a Number) vom datensatz fallen \n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus der Beschreibung der Daten wissen wir, dass es sich um die Ziffern und die dazugehörigen Bilder der Größe 16x16 Pixel handelt. Also in jeder Zeile steht in der 0-ten Spalte die Ziffer und in den folgenden 256 Spalten die einzelnen Pixel der Bilder. Also visualisieren wir diese nun einmal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(20, 10))\n",
    "for i in range(0,10):\n",
    "    image = getNumberImage(cleaned_data,i,'mean')\n",
    "\n",
    "    # Call signature: subplot(nrows, ncols, index, **kwargs)\n",
    "    plt.subplot(2,5, 1 + i)\n",
    "    plt.imshow(image, cmap='hot', interpolation='none')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verschiedene Ansätze mit K-Means zu Clustern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir nehmen an, dass jeder Bildpunkt ein Wert in dem 256 dimensionalen Raum der reelen Zahlen ist. In diesem Raum haben wir wie im 1-, 2-, oder 3-Dimensionalen auch das euklidische Abstandsmaß."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = cleaned_data.iloc[:,1:].values\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set targets\n",
    "exp = Experiment(cleaned_data[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add imputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image = np.zeros((10,256))\n",
    "for i in range(0,10):\n",
    "    image = getNumber(cleaned_data, i).mean()\n",
    "    init_image[i,:] = image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input featurs are the individual pixels of the image - without transformation\n",
    "exp.add_input_data('256_pixel', input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input featurs are the individual pixels of the image - quantized\n",
    "exp.add_input_data('256_pixel_quant', (input_data * 2).round() / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input featurs are the individual pixels of the image - quantized\n",
    "exp.add_input_data('256_pixel_mult', (input_data * 4).round() / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.add_config(KMeans(n_clusters=10), 'KMeans_10', '256_pixel')\n",
    "exp.add_config(KMeans(n_clusters=10, init=init_image, n_init=1), 'KMeans_10_init', '256_pixel')\n",
    "exp.add_config(KMeans(n_clusters=10), 'KMeans_10_Q', '256_pixel_quant')\n",
    "exp.add_config(KMeans(n_clusters=10), 'KMeans_10_Q4', '256_pixel_mult')\n",
    "exp.add_config(KMeans(n_clusters=10, n_init=20), 'KMeans__10__n_init20', '256_pixel')\n",
    "exp.add_config(GaussianMixture(n_components=10, covariance_type='full'), 'GMM__10__cov_full', '256_pixel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.get_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.get_config_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.configs[5]['algo'].means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.predict_all(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.get_all_error_rates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = exp.get_results()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
